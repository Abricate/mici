<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Mici - Python implementations of manifold MCMC methods</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#features">Features</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#why-mici">Why Mici?</a></li>
<li><a href="#related-projects">Related projects</a></li>
<li><a href="#overview-of-package">Overview of package</a></li>
<li><a href="#notebooks">Notebooks</a></li>
<li><a href="#example-sampling-on-a-torus">Example: sampling on a torus</a></li>
<li><a href="#references">References</a></li>
</ul>
</nav>
<img src='images/mici-logo-rectangular.svg' width='400px'/>

<p class='badges'>
  <a href="https://badge.fury.io/py/mici">
    <img src="https://badge.fury.io/py/mici.svg" alt="PyPI version"/>
  </a>
  <a href="https://matt-graham.github.io/mici/docs">
    <img src="https://img.shields.io/badge/API_docs-grey.svg" 
         alt="API documentation"/>
  </a>
  <a href="https://zenodo.org/badge/latestdoi/52494384">
    <img src="https://zenodo.org/badge/52494384.svg" alt="DOI"/>
  </a>
  <img src="images/coverage.svg" alt="Coverage"/>
  <img src="https://github.com/matt-graham/mici/workflows/Tests/badge.svg" alt="Test status" />
</p>

<p><strong>Mici</strong> is a Python package providing implementations of <em>Markov chain Monte Carlo</em> (MCMC) methods for approximate inference in probabilistic models, with a particular focus on MCMC methods based on simulating Hamiltonian dynamics on a manifold.</p>
<h2 id="features">Features</h2>
<p>Key features include</p>
<ul>
<li>a modular design allowing use of a wide range of inference algorithms by mixing and matching different components, and making it easy to extend the package,</li>
<li>a pure Python code base with minimal dependencies, allowing easy integration within other code,</li>
<li>implementations of MCMC methods for sampling from distributions on embedded manifolds implicitly-defined by a constraint equation and distributions on Riemannian manifolds with a user-specified metric,</li>
<li>computationally efficient inference via transparent caching of the results of expensive operations and intermediate results calculated in derivative computations allowing later reuse without recalculation,</li>
<li>memory efficient inference for large models by memory-mapping chains to disk, allowing long runs on large models without hitting memory issues.</li>
</ul>
<h2 id="installation">Installation</h2>
<p>To install and use Mici the minimal requirements are a Python 3.6+ environment with <a href="http://www.numpy.org/">NumPy</a> and <a href="https://www.scipy.org">SciPy</a> installed. The latest Mici release on PyPI (and its dependencies) can be installed in the current Python environment by running</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1"></a><span class="ex">pip</span> install mici</span></code></pre></div>
<p>To instead install the latest development version from the <code>master</code> branch on Github run</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1"></a><span class="ex">pip</span> install git+https://github.com/matt-graham/mici</span></code></pre></div>
<p>If available in the installed Python environment the following additional packages provide extra functionality and features</p>
<ul>
<li><a href="https://github.com/HIPS/autograd">Autograd</a>: if available Autograd will be used to automatically compute the required derivatives of the model functions (providing they are specified using functions from the <code>autograd.numpy</code> and <code>autograd.scipy</code> interfaces). To sample chains in parallel using <code>autograd</code> functions you also need to install <a href="https://github.com/uqfoundation/multiprocess">multiprocess</a>. This will cause <code>multiprocess.Pool</code> to be used in preference to the in-built <code>mutiprocessing.Pool</code> for parallelisation as multiprocess supports serialisation (via <a href="https://github.com/uqfoundation/dill">dill</a>) of a much wider range of types, including of Autograd generated functions. Both Autograd and multiprocess can be installed alongside Mici by running <code>pip install mici[autodiff]</code>.</li>
<li><a href="https://arviz-devs.github.io/arviz/index.html#">ArviZ</a>: if ArviZ is available the traces (dictionary) output of a sampling run can be directly converted to an <code>arviz.InferenceData</code> container object using <code>arviz.convert_to_inference_data</code> or implicitly converted by passing the traces dictionary as the <code>data</code> argument <a href="https://arviz-devs.github.io/arviz/api.html">to ArviZ API functions</a>, allowing straightforward use of the ArviZ's extensive visualisation and diagnostic functions.</li>
</ul>
<h2 id="why-mici">Why Mici?</h2>
<p>Mici is named for <a href="https://en.wikipedia.org/wiki/Augusta_H._Teller">Augusta 'Mici' Teller</a>, who along with <a href="https://en.wikipedia.org/wiki/Arianna_W._Rosenbluth">Arianna Rosenbluth</a> developed the code for the <a href="https://en.wikipedia.org/wiki/MANIAC_I">MANIAC I</a> computer used in the seminal paper <a href="https://doi.org/10.1063%2F1.1699114"><em>Equations of State Calculations by Fast Computing Machines</em></a> which introduced the first example of a Markov chain Monte Carlo method.</p>
<h2 id="related-projects">Related projects</h2>
<p>Other Python packages for performing MCMC inference include <a href="https://github.com/pymc-devs/pymc3">PyMC3</a>, <a href="https://github.com/stan-dev/pystan">PyStan</a> (the Python interface to <a href="http://mc-stan.org/">Stan</a>), <a href="https://github.com/pyro-ppl/pyro">Pyro</a> / <a href="https://github.com/pyro-ppl/numpyro">NumPyro</a>, <a href="https://github.com/tensorflow/probability">TensorFlow Probability</a>, <a href="https://github.com/dfm/emcee">emcee</a> and <a href="https://github.com/mcleonard/sampyl">Sampyl</a>.</p>
<p>Unlike PyMC3, PyStan, (Num)Pyro and TensorFlow Probability which are complete probabilistic programming frameworks including functionality for definining a probabilistic model / program, but like emcee and Sampyl, Mici is solely focussed on providing implementations of inference algorithms, with the user expected to be able to define at a minimum a function specifying the negative log (unnormalized) density of the distribution of interest.</p>
<p>Further while PyStan, (Num)Pyro and TensorFlow Probability all push the sampling loop into external compiled non-Python code, in Mici the sampling loop is run directly within Python. This has the consequence that for small models in which the negative log density of the target distribution and other model functions are cheap to evaluate, the interpreter overhead in iterating over the chains in Python can dominate the computational cost, making sampling much slower than packages which outsource the sampling loop to a efficient compiled implementation.</p>
<h2 id="overview-of-package">Overview of package</h2>
<p>API documentation for the package is available <a href="https://matt-graham.github.io/mici/docs">here</a>. The three main user-facing modules within the <code>mici</code> package are the <code>systems</code>, <code>integrators</code> and <code>samplers</code> modules and you will generally need to create an instance of one class from each module.</p>
<p><a href="https://matt-graham.github.io/mici/docs/systems.html"><code>mici.systems</code></a> - Hamiltonian systems encapsulating model functions and their derivatives</p>
<ul>
<li><code>EuclideanMetricSystem</code> - systems with a metric on the position space with a constant matrix representation,</li>
<li><code>GaussianEuclideanMetricSystem</code> - systems in which the target distribution is defined by a density with respect to the standard Gaussian measure on the position space allowing analytically solving for flow corresponding to the quadratic components of Hamiltonian (<a href="#shababa2014split">Shahbaba et al., 2014</a>),</li>
<li><code>RiemannianMetricSystem</code> - systems with a metric on the position space with a position-dependent matrix representation (<a href="#girolami2011riemann">Girolami and Calderhead, 2011</a>),</li>
<li><code>SoftAbsRiemannianMetricSystem</code> - system with <em>SoftAbs</em> eigenvalue-regularized Hessian of negative log target density as metric matrix representation (<a href="#betancourt2013general">Betancourt, 2013</a>),</li>
<li><code>DenseConstrainedEuclideanMetricSystem</code> - Euclidean-metric system subject to holonomic constraints (<a href="#hartmann2005constrained">Hartmann and Schütte, 2005</a>; <a href="#brubaker2012family">Brubaker, Salzmann and Urtasun, 2012</a>; <a href="#lelievre2019hybrid">Lelièvre, Rousset and Stoltz, 2019</a>) with a dense constraint function Jacobian matrix,</li>
</ul>
<p><a href="https://matt-graham.github.io/mici/docs/integrators.html"><code>mici.integrators</code></a> - symplectic integrators for Hamiltonian dynamics</p>
<ul>
<li><code>LeapfrogIntegrator</code> - explicit leapfrog (Störmer-Verlet) integrator for separable Hamiltonian systems (<a href="#leimkuhler2004simulating">Leimkulher and Reich, 2004</a>),</li>
<li><code>ImplicitLeapfrogIntegrator</code> - implicit (or generalized) leapfrog integrator for non-separable Hamiltonian systems (<a href="#leimkuhler2004simulating">Leimkulher and Reich, 2004</a>),</li>
<li><code>ConstrainedLeapfrogIntegrator</code> - constrained leapfrog integrator for Hamiltonian systems subject to holonomic constraints (<a href="#andersen1983rattle">Andersen, 1983</a>; <a href="#leimkuhler1994symplectic">Leimkuhler and Reich, 1994</a>).</li>
</ul>
<p><a href="https://matt-graham.github.io/mici/docs/samplers.html"><code>mici.samplers</code></a> - MCMC samplers for peforming inference</p>
<ul>
<li><code>StaticMetropolisHMC</code> - static integration time Hamiltonian Monte Carlo with Metropolis accept step (<a href="duane1987hybrid">Duane et al., 1987</a>),</li>
<li><code>RandomMetropolisHMC</code> - random integration time Hamiltonian Monte Carlo with Metropolis accept step (<a href="#mackenzie1989improved">Mackenzie, 1989</a>),</li>
<li><code>DynamicSliceHMC</code> - dynamic integration time Hamiltonian Monte Carlo with slice sampling from trajectory, equivalent to the original 'NUTS' algorithm (<a href="#hoffman2014nouturn">Hoffman and Gelman, 2014</a>).</li>
<li><code>DynamicMultinomialHMC</code> - dynamic integration time Hamiltonian Monte Carlo with multinomial sampling from trajectory, equivalent to the current default MCMC algorithm in <a href="https://mc-stan.org/">Stan</a> (<a href="#hoffman2014nouturn">Hoffman and Gelman, 2014</a>; <a href="#betancourt2017conceptual">Betancourt, 2017</a>).</li>
</ul>
<h2 id="notebooks">Notebooks</h2>
<p>The manifold MCMC methods implemented in Mici have been used in several research projects. Below links are provided to a selection of Jupyter notebooks associated with these projects as demonstrations of how to use Mici and to illustrate some of the settings in which manifold MCMC methods can be computationally advantageous.</p>
<table>
  <tr>
    <th colspan="2"><img src='https://raw.githubusercontent.com/jupyter/design/master/logos/Favicon/favicon.svg?sanitize=true' width="15" style="vertical-align:text-bottom; margin-right: 5px;"/> <a href="https://github.com/thiery-lab/manifold_lifting">Manifold lifting: MCMC in the vanishing noise regime</a></th>
  </tr>
  <tr>
    <td>Open non-interactive version with nbviewer</td>
    <td>
      <a href="https://nbviewer.jupyter.org/github/thiery-lab/manifold_lifting/blob/master/notebooks/Two-dimensional_example.ipynb">
        <img src="https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true" width="109" alt="Render with nbviewer"  style="vertical-align:text-bottom" />
      </a>
    </td>
  </tr>
  <tr>
    <td>Open interactive version with Binder</td>
    <td>
      <a href="https://mybinder.org/v2/gh/thiery-lab/manifold_lifting/master?filepath=notebooks%2FTwo-dimensional_example.ipynb">
        <img src="https://mybinder.org/badge_logo.svg" alt="Launch with Binder"  style="vertical-align:text-bottom"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>Open interactive version with Google Colab</td>
    <td>
      <a href="https://colab.research.google.com/github/thiery-lab/manifold_lifting/blob/master/notebooks/Two-dimensional_example.ipynb">
        <img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" style="vertical-align:text-bottom">
       </a> 
    </td>
  </tr>
</table>

<table>
  <tr>
    <th colspan="2"><img src='https://raw.githubusercontent.com/jupyter/design/master/logos/Favicon/favicon.svg?sanitize=true' width="15" style="vertical-align:text-bottom; margin-right: 5px;"/><a href="https://github.com/thiery-lab/manifold-mcmc-for-diffusions">Manifold MCMC methods for inference in diffusion models</a></th>
  </tr>
  <tr>
    <td>Open non-interactive version with nbviewer</td>
    <td>
      <a href="https://nbviewer.jupyter.org/github/thiery-lab/manifold-mcmc-for-diffusions/blob/master/FitzHugh-Nagumo_example.ipynb">
        <img src="https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true" width="109" alt="Render with nbviewer"  style="vertical-align:text-bottom" />
      </a>
    </td>
  </tr>
  <tr>
    <td>Open interactive version with Binder</td>
    <td>
      <a href="https://mybinder.org/v2/gh/thiery-lab/manifold-mcmc-for-diffusions/master?filepath=FitzHugh-Nagumo_example.ipynb">
        <img src="https://mybinder.org/badge_logo.svg" alt="Launch with Binder"  style="vertical-align:text-bottom"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>Open interactive version with Google Colab</td>
    <td>
      <a href="https://colab.research.google.com/github/thiery-lab/manifold-mcmc-for-diffusions/blob/master/FitzHugh-Nagumo_example.ipynb">
        <img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" style="vertical-align:text-bottom">
       </a> 
    </td>
  </tr>
</table>

<h2 id="example-sampling-on-a-torus">Example: sampling on a torus</h2>
<img src='images/torus-samples.gif' width='360px'/>

<p>A simple complete example of using the package to compute approximate samples from a distribution on a two-dimensional torus embedded in a three-dimensional space is given below. The computed samples are visualized in the animation above. Here we use <code>autograd</code> to automatically construct functions to calculate the required derivatives (gradient of negative log density of target distribution and Jacobian of constraint function), sample four chains in parallel using <code>multiprocess</code>, use <code>arviz</code> to calculate diagnostics and use <code>matplotlib</code> to plot the samples.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode Python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> mici <span class="im">import</span> systems, integrators, samplers</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="im">import</span> arviz</span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="co"># Define fixed model parameters</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>R <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># toroidal radius ∈ (0, ∞)</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>r <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># poloidal radius ∈ (0, R)</span></span>
<span id="cb3-11"><a href="#cb3-11"></a>α <span class="op">=</span> <span class="fl">0.9</span>  <span class="co"># density fluctuation amplitude ∈ [0, 1)</span></span>
<span id="cb3-12"><a href="#cb3-12"></a></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co"># Define constraint function such that the set {q : constr(q) == 0} is a torus</span></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="kw">def</span> constr(q):</span>
<span id="cb3-15"><a href="#cb3-15"></a>    x, y, z <span class="op">=</span> q.T</span>
<span id="cb3-16"><a href="#cb3-16"></a>    <span class="cf">return</span> np.stack([((x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span><span class="fl">0.5</span> <span class="op">-</span> R)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> z<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> r<span class="op">**</span><span class="dv">2</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-17"><a href="#cb3-17"></a></span>
<span id="cb3-18"><a href="#cb3-18"></a><span class="co"># Define negative log density for the target distribution on torus</span></span>
<span id="cb3-19"><a href="#cb3-19"></a><span class="co"># (with respect to 2D &#39;area&#39; measure for torus)</span></span>
<span id="cb3-20"><a href="#cb3-20"></a><span class="kw">def</span> neg_log_dens(q):</span>
<span id="cb3-21"><a href="#cb3-21"></a>    x, y, z <span class="op">=</span> q.T</span>
<span id="cb3-22"><a href="#cb3-22"></a>    θ <span class="op">=</span> np.arctan2(y, x)</span>
<span id="cb3-23"><a href="#cb3-23"></a>    ϕ <span class="op">=</span> np.arctan2(z, x <span class="op">/</span> np.cos(θ) <span class="op">-</span> R)</span>
<span id="cb3-24"><a href="#cb3-24"></a>    <span class="cf">return</span> np.log1p(r <span class="op">*</span> np.cos(ϕ) <span class="op">/</span> R) <span class="op">-</span> np.log1p(np.sin(<span class="dv">4</span><span class="op">*</span>θ) <span class="op">*</span> np.cos(ϕ) <span class="op">*</span> α)</span>
<span id="cb3-25"><a href="#cb3-25"></a></span>
<span id="cb3-26"><a href="#cb3-26"></a><span class="co"># Specify constrained Hamiltonian system with default identity metric</span></span>
<span id="cb3-27"><a href="#cb3-27"></a>system <span class="op">=</span> systems.DenseConstrainedEuclideanMetricSystem(neg_log_dens, constr)</span>
<span id="cb3-28"><a href="#cb3-28"></a></span>
<span id="cb3-29"><a href="#cb3-29"></a><span class="co"># System is constrained therefore use constrained leapfrog integrator</span></span>
<span id="cb3-30"><a href="#cb3-30"></a>integrator <span class="op">=</span> integrators.ConstrainedLeapfrogIntegrator(system)</span>
<span id="cb3-31"><a href="#cb3-31"></a></span>
<span id="cb3-32"><a href="#cb3-32"></a><span class="co"># Seed a random number generator</span></span>
<span id="cb3-33"><a href="#cb3-33"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb3-34"><a href="#cb3-34"></a></span>
<span id="cb3-35"><a href="#cb3-35"></a><span class="co"># Use dynamic integration-time HMC implementation as MCMC sampler</span></span>
<span id="cb3-36"><a href="#cb3-36"></a>sampler <span class="op">=</span> samplers.DynamicMultinomialHMC(system, integrator, rng)</span>
<span id="cb3-37"><a href="#cb3-37"></a></span>
<span id="cb3-38"><a href="#cb3-38"></a><span class="co"># Sample initial positions on torus using parameterisation (θ, ϕ) ∈ [0, 2π)²</span></span>
<span id="cb3-39"><a href="#cb3-39"></a><span class="co"># x, y, z = (R + r * cos(ϕ)) * cos(θ), (R + r * cos(ϕ)) * sin(θ), r * sin(ϕ)</span></span>
<span id="cb3-40"><a href="#cb3-40"></a>n_chain <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb3-41"><a href="#cb3-41"></a>θ_init, ϕ_init <span class="op">=</span> rng.uniform(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span> np.pi, size<span class="op">=</span>(<span class="dv">2</span>, n_chain))</span>
<span id="cb3-42"><a href="#cb3-42"></a>q_init <span class="op">=</span> np.stack([</span>
<span id="cb3-43"><a href="#cb3-43"></a>    (R <span class="op">+</span> r <span class="op">*</span> np.cos(ϕ_init)) <span class="op">*</span> np.cos(θ_init),</span>
<span id="cb3-44"><a href="#cb3-44"></a>    (R <span class="op">+</span> r <span class="op">*</span> np.cos(ϕ_init)) <span class="op">*</span> np.sin(θ_init),</span>
<span id="cb3-45"><a href="#cb3-45"></a>    r <span class="op">*</span> np.sin(ϕ_init)], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-46"><a href="#cb3-46"></a></span>
<span id="cb3-47"><a href="#cb3-47"></a><span class="co"># Define function to extract variables to trace during sampling</span></span>
<span id="cb3-48"><a href="#cb3-48"></a><span class="kw">def</span> trace_func(state):</span>
<span id="cb3-49"><a href="#cb3-49"></a>    x, y, z <span class="op">=</span> state.pos</span>
<span id="cb3-50"><a href="#cb3-50"></a>    <span class="cf">return</span> {<span class="st">&#39;x&#39;</span>: x, <span class="st">&#39;y&#39;</span>: y, <span class="st">&#39;z&#39;</span>: z}</span>
<span id="cb3-51"><a href="#cb3-51"></a></span>
<span id="cb3-52"><a href="#cb3-52"></a><span class="co"># Sample 4 chains in parallel with 500 adaptive warm up iterations in which the</span></span>
<span id="cb3-53"><a href="#cb3-53"></a><span class="co"># integrator step size is tuned, followed by 2000 non-adaptive iterations</span></span>
<span id="cb3-54"><a href="#cb3-54"></a>final_states, traces, stats <span class="op">=</span> sampler.sample_chains_with_adaptive_warm_up(</span>
<span id="cb3-55"><a href="#cb3-55"></a>    n_warm_up_iter<span class="op">=</span><span class="dv">500</span>, n_main_iter<span class="op">=</span><span class="dv">2000</span>, init_states<span class="op">=</span>q_init, n_process<span class="op">=</span><span class="dv">4</span>, </span>
<span id="cb3-56"><a href="#cb3-56"></a>    trace_funcs<span class="op">=</span>[trace_func])</span>
<span id="cb3-57"><a href="#cb3-57"></a></span>
<span id="cb3-58"><a href="#cb3-58"></a><span class="co"># Print average accept probability and number of integrator steps per chain</span></span>
<span id="cb3-59"><a href="#cb3-59"></a><span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(n_chain):</span>
<span id="cb3-60"><a href="#cb3-60"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Chain </span><span class="sc">{c}</span><span class="ss">:&quot;</span>)</span>
<span id="cb3-61"><a href="#cb3-61"></a>    <span class="bu">print</span>(<span class="ss">f&quot;  Average accept prob. = </span><span class="sc">{</span>stats[<span class="st">&#39;accept_stat&#39;</span>][c]<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-62"><a href="#cb3-62"></a>    <span class="bu">print</span>(<span class="ss">f&quot;  Average number steps = </span><span class="sc">{</span>stats[<span class="st">&#39;n_step&#39;</span>][c]<span class="sc">.</span>mean()<span class="sc">:.1f}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-63"><a href="#cb3-63"></a>    </span>
<span id="cb3-64"><a href="#cb3-64"></a><span class="co"># Print summary statistics and diagnostics computed using ArviZ</span></span>
<span id="cb3-65"><a href="#cb3-65"></a><span class="bu">print</span>(arviz.summary(traces))</span>
<span id="cb3-66"><a href="#cb3-66"></a></span>
<span id="cb3-67"><a href="#cb3-67"></a><span class="co"># Visualize concatentated chain samples as animated 3D scatter plot   </span></span>
<span id="cb3-68"><a href="#cb3-68"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb3-69"><a href="#cb3-69"></a>ax <span class="op">=</span> Axes3D(fig, [<span class="fl">0.</span>, <span class="fl">0.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>], proj_type<span class="op">=</span><span class="st">&#39;ortho&#39;</span>)</span>
<span id="cb3-70"><a href="#cb3-70"></a>points_3d, <span class="op">=</span> ax.plot(<span class="op">*</span>(np.concatenate(traces[k]) <span class="cf">for</span> k <span class="kw">in</span> <span class="st">&#39;xyz&#39;</span>), <span class="st">&#39;.&#39;</span>, ms<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-71"><a href="#cb3-71"></a>ax.axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb3-72"><a href="#cb3-72"></a><span class="cf">for</span> set_lim <span class="kw">in</span> [ax.set_xlim, ax.set_ylim, ax.set_zlim]:</span>
<span id="cb3-73"><a href="#cb3-73"></a>    set_lim((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb3-74"><a href="#cb3-74"></a></span>
<span id="cb3-75"><a href="#cb3-75"></a><span class="kw">def</span> update(i):</span>
<span id="cb3-76"><a href="#cb3-76"></a>    angle <span class="op">=</span> <span class="dv">45</span> <span class="op">*</span> (np.sin(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> i <span class="op">/</span> <span class="dv">60</span>) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb3-77"><a href="#cb3-77"></a>    ax.view_init(elev<span class="op">=</span>angle, azim<span class="op">=</span>angle)</span>
<span id="cb3-78"><a href="#cb3-78"></a>    <span class="cf">return</span> (points_3d,)</span>
<span id="cb3-79"><a href="#cb3-79"></a></span>
<span id="cb3-80"><a href="#cb3-80"></a>anim <span class="op">=</span> animation.FuncAnimation(fig, update, frames<span class="op">=</span><span class="dv">60</span>, interval<span class="op">=</span><span class="dv">100</span>, blit<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<h2 id="references">References</h2>
<ol>
<li><a id='andersen1983rattle'></a> Andersen, H.C., 1983. RATTLE: A “velocity” version of the SHAKE algorithm for molecular dynamics calculations. <em>Journal of Computational Physics</em>, 52(1), pp.24-34. <a href="https://doi.org/10.1016/0021-9991(83)90014-1"><img src="https://zenodo.org/badge/DOI/10.1016/0021-9991(83)90014-1.svg" alt="DOI:10.1016/0021-9991(83)90014-1" /></a></li>
<li><a id='duane1987hybrid'></a> Duane, S., Kennedy, A.D., Pendleton, B.J. and Roweth, D., 1987. Hybrid Monte Carlo. <em>Physics letters B</em>, 195(2), pp.216-222. <a href="https://doi.org/10.1016/0370-2693(87)91197-X"><img src="https://zenodo.org/badge/DOI/10.1016/0370-2693(87)91197-X.svg" alt="DOI:10.1016/0370-2693(87)91197-X" /></a></li>
<li><a id='mackenzie1989improved'></a> Mackenzie, P.B., 1989. An improved hybrid Monte Carlo method. <em>Physics Letters B</em>, 226(3-4), pp.369-371. <a href="https://doi.org/10.1016/0370-2693(89)91212-4"><img src="https://zenodo.org/badge/DOI/10.1016/0370-2693(89)91212-4.svg" alt="DOI:10.1016/0370-2693(89)91212-4" /></a></li>
<li><a id='horowitz1991generalized'></a> Horowitz, A.M., 1991. A generalized guided Monte Carlo algorithm. <em>Physics Letters B</em>, 268(CERN-TH-6172-91), pp.247-252. <a href="https://doi.org/10.1016/0370-2693(91)90812-5"><img src="https://zenodo.org/badge/DOI/10.1016/0370-2693(91)90812-5.svg" alt="DOI:10.1016/0370-2693(91)90812-5" /></a></li>
<li><a id='leimkuhler1994symplectic'></a> Leimkuhler, B. and Reich, S., 1994. Symplectic integration of constrained Hamiltonian systems. <em>Mathematics of Computation</em>, 63(208), pp.589-605. <a href="https://doi.org/10.2307/2153284"><img src="https://zenodo.org/badge/DOI/10.2307/2153284.svg" alt="DOI:10.2307/2153284" /></a></li>
<li><a id='leimkuhler2004simulating'></a> Leimkuhler, B. and Reich, S., 2004. Simulating Hamiltonian dynamics (Vol. 14). <em>Cambridge University Press</em>. <a href="https://doi.org/10.1017/CBO9780511614118"><img src="https://zenodo.org/badge/DOI/10.1017/CBO9780511614118.svg" alt="DOI:10.1017/CBO9780511614118" /></a></li>
<li><a id='hartmann2005constrained'></a> Hartmann, C. and Schütte, C., 2005. A constrained hybrid Monte‐Carlo algorithm and the problem of calculating the free energy in several variables. <em>ZAMM ‐ Journal of Applied Mathematics and Mechanics</em>, 85(10), pp.700-710. <a href="https://doi.org/10.1002/zamm.200410218"><img src="https://zenodo.org/badge/DOI/10.1002/zamm.200410218.svg" alt="DOI:10.1002/zamm.200410218" /></a></li>
<li><a id='girolami2011riemann'></a> Girolami, M. and Calderhead, B., 2011. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 73(2), pp.123-214. <a href="https://doi.org/10.1111/j.1467-9868.2010.00765.x"><img src="https://zenodo.org/badge/DOI/10.1111/j.1467-9868.2010.00765.x.svg" alt="DOI:10.1111/j.1467-9868.2010.00765.x" /></a></li>
<li><a id='brubaker2012family'></a> Brubaker, M., Salzmann, M. and Urtasun, R., 2012. A family of MCMC methods on implicitly defined manifolds. In <em>Artificial intelligence and statistics</em> (pp. 161-172). <a href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.417.6111"><img src="http://img.shields.io/badge/CiteSeerX-10.1.1.417.6111-blue.svg" alt="CiteSeerX:10.1.1.417.6111" /></a></li>
<li><a id='betancourt2013general'></a> Betancourt, M., 2013. A general metric for Riemannian manifold Hamiltonian Monte Carlo. In <em>Geometric science of information</em> (pp. 327-334). <a href="https://doi.org/10.1007/978-3-642-40020-9_35"><img src="https://zenodo.org/badge/DOI/10.1007/978-3-642-40020-9_35.svg" alt="DOI:10.1007/978-3-642-40020-9_35" /></a> <a href="https://arxiv.org/abs/1212.4693"><img src="http://img.shields.io/badge/arXiv-1212.4693-B31B1B.svg" alt="arXiv:1212.4693" /></a></li>
<li><a id='hoffman2014nouturn'></a> Hoffman, M.D. and Gelman, A., 2014. The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. <em>Journal of Machine Learning Research</em>, 15(1), pp.1593-1623. <a href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.220.8395"><img src="http://img.shields.io/badge/CiteSeerX-10.1.1.220.8395-blue.svg" alt="CiteSeerX:10.1.1.220.8395" /></a> <a href="https://arxiv.org/abs/1111.4246"><img src="http://img.shields.io/badge/arXiv-1111.4246-B31B1B.svg" alt="arXiv:1111.4246" /></a></li>
<li><a id='shababa2014split'></a> Shahbaba, B., Lan, S., Johnson, W.O. and Neal, R.M., 2014. Split Hamiltonian Monte Carlo. <em>Statistics and Computing</em>, 24(3), pp.339-349. <a href="https://doi.org/10.1007/s11222-012-9373-1"><img src="https://zenodo.org/badge/DOI/10.1007/s11222-012-9373-1.svg" alt="DOI:10.1007/s11222-012-9373-1" /></a> <a href="https://arxiv.org/abs/1106.5941"><img src="http://img.shields.io/badge/arXiv-1106.5941-B31B1B.svg" alt="arXiv:1106.5941" /></a></li>
<li><a id='betancourt2017conceptual'></a> Betancourt, M., 2017. A conceptual introduction to Hamiltonian Monte Carlo. <a href="https://arxiv.org/abs/1701.02434"><img src="http://img.shields.io/badge/arXiv-1701.02434-B31B1B.svg" alt="arXiv:1701.02434" /></a></li>
<li><a id='lelievre2019hybrid'></a> Lelièvre, T., Rousset, M. and Stoltz, G., 2019. Hybrid Monte Carlo methods for sampling probability measures on submanifolds. In <em>Numerische Mathematik</em>, 143(2), (pp.379-421). <a href="https://doi.org/10.1007/s00211-019-01056-4"><img src="https://zenodo.org/badge/DOI/10.1007/s00211-019-01056-4.svg" alt="DOI:10.1007/s00211-019-01056-4" /></a> <a href="https://arxiv.org/abs/1807.02356"><img src="http://img.shields.io/badge/arXiv-1807.02356-B31B1B.svg" alt="arXiv:1807.02356" /></a></li>
</ol>
</body>
</html>
